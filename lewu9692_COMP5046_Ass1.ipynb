{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lewu9692_COMP5046_Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO"
      },
      "source": [
        "[link text](https://)***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "outputId": "13605be5-ea1a-4286-da2e-dcfcfbc5522c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_train.csv')\n",
        "\n",
        "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"imdb_train.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test.csv\")\n",
        "\n",
        "reviews_train = df_train['review'].tolist()\n",
        "sentiments_train = df_train['sentiment'].tolist()\n",
        "reviews_test = df_test['review'].tolist()\n",
        "sentiments_test = df_test['sentiment'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(reviews_train))\n",
        "print(\"Testing set number:\",len(reviews_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 25000\n",
            "Testing set number: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe"
      },
      "source": [
        "*Pre-processing techniques:*\n",
        "\n",
        "1.   removing numbers\n",
        "2.   converting to lowercase\n",
        "3.   removing stop words\n",
        "5.   tokenizer.TweetTokenizer is applied as it can extract url\n",
        "6.   removing html tag like \\<br /\\>\n",
        "7.   expanding contraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12"
      },
      "source": [
        "# import necessary libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.stem.porter import *\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJuhfJ8DR987"
      },
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejFPZHuoNr_c"
      },
      "source": [
        "def multiple_replace(text,ad):\n",
        "    \"\"\"\n",
        "    This function is used to replace multiple string at a time and accept a dict as mapping\n",
        "    \"\"\"\n",
        "    rx=re.compile('|'.join([re.escape(x) for x in ad.keys()]))\n",
        "    def one_xlat(match):\n",
        "        return ad[match.group(0)]\n",
        "    return rx.sub(one_xlat,text)\n",
        "\n",
        "\n",
        "tknzr=TweetTokenizer()\n",
        "tag_dict={'<br />':''}\n",
        "stopwords_dict={word:'' for word in sw.words()}\n",
        "dicts={**contraction_dict, **tag_dict}\n",
        "reviews_train = [multiple_replace(text.lower(),dicts) for text in reviews_train]\n",
        "reviews_train = [list(filter(lambda x: re.fullmatch(r'[a-z\\']+',x) #this regular expression is to remove punctuation, url and keep possessive\n",
        "                               and x not in stopwords_dict #remove stop words\n",
        "                               ,tknzr.tokenize(text)))\n",
        "                for text in reviews_train              \n",
        "          ]           \n",
        "\n",
        "reviews_test=[multiple_replace(text.lower(),dicts) for text in reviews_test]\n",
        "reviews_test=[list(filter(lambda x: re.fullmatch(r'[a-z\\']+',x) \n",
        "                               and x not in stopwords_dict\n",
        "                               ,tknzr.tokenize(text)))\n",
        "              for text in reviews_test              \n",
        "          ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3LQsRHSb-57",
        "outputId": "9041a898-38ab-4013-8e41-7154e404b184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# encode labels to 0-negtive and 1-positive\n",
        "labels = np.unique(sentiments_train)\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(labels)\n",
        "label_train_n = lEnc.transform(sentiments_train)\n",
        "label_test_n = lEnc.transform(sentiments_test)\n",
        "numClass = len(labels)\n",
        "\n",
        "print(labels)\n",
        "print(lEnc.transform(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg' 'pos']\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-"
      },
      "source": [
        "Word2Vec with Skip-gram is implemented. Word2Vec with Skip-gram normally has higher accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg"
      },
      "source": [
        "This section uses [reviews_train] from section 1 \n",
        "\n",
        "and <b>stemming</b> is implemented  as lemmatisation does not work well when parts of speech of words are not given.\n",
        "\n",
        "Besides,the vocabulary size after stemming is less than lemmatisation,which can reduce training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu"
      },
      "source": [
        "# convert all sentences to unique word list. \n",
        "stemmer = PorterStemmer()\n",
        "word_sequence = [stemmer.stem(word) for reviews in reviews_train for word in reviews]\n",
        "word_list = [stemmer.stem(word) for reviews in reviews_train for word in reviews]\n",
        "word_list = list(set(word_list))\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "def prepare_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "        input_temp = [0]*voc_size\n",
        "        input_temp[data[i][0]] = 1\n",
        "        random_inputs.append(input_temp)  \n",
        "        random_labels.append(data[i][1])  \n",
        "\n",
        "    return np.array(random_inputs), np.array(random_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlqVsfPH1CRC",
        "outputId": "41d5f2d6-9a39-4bd4-b5d7-6ad9bd05be3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(word_list))\n",
        "print(len(skip_grams))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55198\n",
            "5616570\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMOIgGc0t-p-"
      },
      "source": [
        "# Setting hyperparameters\n",
        "# when learning_rate equals to 0.1, the training loss is very unstable\n",
        "# when learning_rate equals to 0.01, the training loss descreases stably\n",
        "# embedding_size=50 won't blow up RAM memory\n",
        "voc_size = len(word_list)\n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "embedding_size = 50\n",
        "epochs=5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.linear1 = nn.Linear(voc_size, embedding_size,bias=False)\n",
        "        self.linear2 = nn.Linear(embedding_size, voc_size,bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.linear1(x)\n",
        "        out = self.linear2(hidden)\n",
        "        return out\n",
        "skip_gram_model = SkipGram().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(skip_gram_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "outputId": "0cbd3bab-8b61-4e87-de8d-9bdafa516859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    inputs,labels = prepare_batch(skip_grams, batch_size)\n",
        "    inputs_torch = torch.from_numpy(inputs).float().to(device)\n",
        "    labels_torch = torch.from_numpy(labels).to(device)\n",
        "    \n",
        "    \n",
        "    skip_gram_model.train()\n",
        "    # 1. zero grad\n",
        "    optimiser.zero_grad()\n",
        "    # 2. forword propagation\n",
        "    outputs=skip_gram_model(inputs_torch)\n",
        "    # 3. calculate loss\n",
        "    loss=criterion(outputs,labels_torch)\n",
        "    # 4. back propagation\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    if epoch % 500 == 499: \n",
        "        print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 500, loss: 8.9848\n",
            "Epoch: 1000, loss: 9.1340\n",
            "Epoch: 1500, loss: 8.7943\n",
            "Epoch: 2000, loss: 8.6207\n",
            "Epoch: 2500, loss: 8.9641\n",
            "Epoch: 3000, loss: 8.6814\n",
            "Epoch: 3500, loss: 8.1079\n",
            "Epoch: 4000, loss: 8.0993\n",
            "Epoch: 4500, loss: 8.3282\n",
            "Epoch: 5000, loss: 8.2813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo"
      },
      "source": [
        "### 2.1.4. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwicNPkIqd1",
        "outputId": "2acdc7ef-9d30-4a16-a1c8-cb0a568f18a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Save model and upload to Google Drive\n",
        "torch.save(skip_gram_model, 'lewu9692_word.pt')\n",
        "uploaded = drive.CreateFile({'title': 'lewu9692_word.pt'})\n",
        "uploaded.SetContentFile('lewu9692_word.pt')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type SkipGram. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1yOjyjEU6AInUXeaObPalPNlinIuxYZ-z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B"
      },
      "source": [
        "### 2.1.5. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh"
      },
      "source": [
        "# download model from Google Drive and load it \n",
        "# to load model successfully, you need to rerun the cells in this section except for training part.\n",
        "id = '1yOjyjEU6AInUXeaObPalPNlinIuxYZ-z'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('lewu9692_word.pt')\n",
        "word_embedding_model = torch.load('lewu9692_word.pt').to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Data Preprocessing for Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbLnN-3GlI1"
      },
      "source": [
        "This section uses same data from [word embedding] section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CUCL1cGlI2"
      },
      "source": [
        "# this section uses the same vocabulary as word embedding section\n",
        "\n",
        "# extract weight from word embedding model\n",
        "weight1 = word_embedding_model.linear1.weight\n",
        "word_embeddings = weight1.detach().T.cpu().numpy()\n",
        "\n",
        "#Assume that we have the following character instances\n",
        "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
        "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
        "\n",
        "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
        "            'v', 'w', 'x', 'y', 'z','\\'']\n",
        "\n",
        "# one-hot encoding and decoding \n",
        "# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n",
        "num_dic = {n: i+1 for i, n in enumerate(char_arr)}\n",
        "num_dic['P'] = 0 #encoding for padding\n",
        "dic_len = len(num_dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglFgQNKxmnz",
        "outputId": "d5f38dce-8a45-42ee-813c-b678dad165e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# there are some very long words but they are minority.\n",
        "# if padding with max length of words, input of model will be huge.\n",
        "# To find a proper length truncating, accumalate amount of each length until it accounts for defined ratio\n",
        "word_length_cnt=Counter(map(len,word_list)).most_common()\n",
        "word_length_cnt.sort()\n",
        "total_amt=0\n",
        "ratio=0.95\n",
        "for idx,(length,amt) in enumerate(word_length_cnt):\n",
        "  if total_amt/len(word_list)<=ratio:\n",
        "    total_amt += amt\n",
        "  else:\n",
        "    break\n",
        "max_word_len = idx\n",
        "max_word_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUhc7TxWPAtn"
      },
      "source": [
        "def add_padding(word):\n",
        "  if len(word)>=max_word_len:\n",
        "    return word[:max_word_len]\n",
        "  else:\n",
        "    return word+'P'*(max_word_len-len(word))\n",
        "def make_batch(seq_data):\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "    \n",
        "    for seq in seq_data:\n",
        "        after_padding=add_padding(seq)\n",
        "        input_data = [num_dic[n] for n in after_padding]\n",
        "        target = word_embeddings[seq_data.index(seq)]\n",
        "        # convert input to one-hot encoding.\n",
        "        # if input is [3, 4, 4]:\n",
        "        # [[ 0,  0,  0,  1,  0,  0,  0, ... 0]\n",
        "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]\n",
        "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]]\n",
        "        input_batch.append(np.eye(dic_len)[input_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, target_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgiOPcsTGlI6"
      },
      "source": [
        "### 2.2.2. Build Character Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NtqFFcjGlI7"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVOLaJz5CnBT"
      },
      "source": [
        "# setting hyperparameters\n",
        "# from previous experience, learning_rate more than 0.01 may result in a big loss\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 5000\n",
        "n_input = dic_len\n",
        "n_class = embedding_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj3YZ3PWGlI8"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True,bidirectional=True, dropout=0.2)\n",
        "        self.linear = nn.Linear(n_hidden*2,n_class)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        \n",
        "        #h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
        "        lstm_out, (h_n,c_n) = self.lstm(sentence)\n",
        "        #concat the last hidden state from two direction\n",
        "        hidden_out =torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        z = self.linear(hidden_out)\n",
        "        log_output = F.log_softmax(z, dim=1)\n",
        "        return log_output,hidden_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46W0zFfWGlI_"
      },
      "source": [
        "### 2.1.4. Train Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWQn-VyNGlJA"
      },
      "source": [
        "# Preparing input\n",
        "input_batch, target_batch = make_batch(word_list)\n",
        "# Convert input into tensors and move them to GPU by uting tensor.to(device)\n",
        "input_batch_torch = torch.from_numpy(np.array(input_batch)).float().to(device)\n",
        "target_batch_torch = torch.from_numpy(np.array(target_batch)).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTfAp0n7PWR-",
        "outputId": "a825f7d6-c837-453b-b27b-189a80899af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Move the model to GPU\n",
        "net = Net().to(device)\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):  \n",
        "    \n",
        "    # Set the flag to training\n",
        "    net.train()\n",
        "    # forward + backward + optimize\n",
        "    outputs,_ = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    # Set the flag to evaluation, which will 'turn off' the dropout\n",
        "    net.eval()\n",
        "    outputs,_ = net(input_batch_torch) \n",
        "    # Evaluation loss and accuracy calculation\n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    if epoch % 500 == 499:\n",
        "      print('Epoch: %d, loss: %.5f' %(epoch + 1, loss.item()))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 500, loss: 15.47548\n",
            "Epoch: 1000, loss: 15.47514\n",
            "Epoch: 1500, loss: 15.47333\n",
            "Epoch: 2000, loss: 15.47294\n",
            "Epoch: 2500, loss: 15.47261\n",
            "Epoch: 3000, loss: 15.47240\n",
            "Epoch: 3500, loss: 15.47225\n",
            "Epoch: 4000, loss: 15.47215\n",
            "Epoch: 4500, loss: 15.47206\n",
            "Epoch: 5000, loss: 15.47199\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Bym9bBGlJE"
      },
      "source": [
        "### 2.1.5. Save Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggTsYIm7GlJF",
        "outputId": "6a311836-5762-47ac-b262-107554de722f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#save model and upload it to Google Drive\n",
        "torch.save(net, 'lewu9692_character.pt')\n",
        "\n",
        "uploaded = drive.CreateFile({'title': 'lewu9692_character.pt'})\n",
        "uploaded.SetContentFile('lewu9692_character.pt')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1m8FAdDTuizqTpNzGbbzDHn5ftarvhyQE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwOI-wIKGlJI"
      },
      "source": [
        "### 2.1.6. Load Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jyj-lOHWWj"
      },
      "source": [
        "# download model from Google Drive and load it\n",
        "# to load model successfully, you need to rerun the cells in this section except for training part.\n",
        "id = '1m8FAdDTuizqTpNzGbbzDHn5ftarvhyQE'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('lewu9692_character.pt')\n",
        "character_embedding_model = torch.load('lewu9692_character.pt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd"
      },
      "source": [
        "## 2.3. Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMJrxx-iOVn"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3xqqbrBRdDd",
        "outputId": "35f95d5f-7fce-400b-a16e-3bab46e93412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# there are some very long reviews but they are minority.\n",
        "# if padding with max length of reviews, input of model will be huge.\n",
        "# To find a proper length truncating, accumalate amount of each length until it accounts for defined ratio of total amount of reviews\n",
        "from collections import Counter\n",
        "cnt=Counter(map(len,reviews_train)).most_common()\n",
        "cnt.sort()\n",
        "\n",
        "total_amt=0\n",
        "ratio=0.8\n",
        "for idx,(length,amt) in enumerate(cnt):\n",
        "  if total_amt/len(reviews_train)<=ratio:\n",
        "    total_amt += amt\n",
        "  else:\n",
        "    break\n",
        "max_review_len = idx\n",
        "max_review_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2"
      },
      "source": [
        "# extract hidding state from character embedding model\n",
        "character_embedding_model.eval()\n",
        "_,hidden_state = character_embedding_model(input_batch_torch)\n",
        "character_embeddings = hidden_state.data\n",
        "\n",
        "# extract weight from word embedding model\n",
        "word_embedding_model.eval()\n",
        "weight1 = word_embedding_model.linear1.weight\n",
        "word_embeddings = weight1.detach().T\n",
        "\n",
        "# concatenate word embeddings and character embeddings\n",
        "embeddings = torch.cat((word_embeddings,character_embeddings),1).cpu().numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK6OdIad6eVz"
      },
      "source": [
        "# this function is to get embedding of reviews from concatenated embeddings\n",
        "def generate_embeddings(sentences,embeddings):\n",
        "\n",
        "  input_embeddings = []\n",
        "  zero_padding = np.zeros((embeddings.shape[1],))\n",
        "  for sentence in tqdm(sentences):\n",
        "    tmp_embeddings = []\n",
        "    for idx in range(max_review_len):\n",
        "      try:\n",
        "        tmp_embeddings.append(embeddings[word_list.index(sentence[idx])])\n",
        "      except:\n",
        "        tmp_embeddings.append(zero_padding)\n",
        "    input_embeddings.append(tmp_embeddings)\n",
        "  return np.array(input_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pia2vgnMYV-b"
      },
      "source": [
        "# please note that this step will cost one hours\n",
        "train_embeddings = generate_embeddings(reviews_train,embeddings)\n",
        "train_target = torch.from_numpy(np.array(label_train_n)).to(device)\n",
        "train_embeddings_torch = torch.from_numpy(train_embeddings).float().to(device) # move embeddings to GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6"
      },
      "source": [
        "# according to the figure in the [evaluation] section, when learning rate equals to 0.01, the model performs better.\n",
        "# and at epoch 1000, f1 score remains stable.\n",
        "n_input = train_embeddings_torch.shape[2]\n",
        "n_hidden = 50\n",
        "n_class = len(labels)\n",
        "total_epoch = 1000\n",
        "learning_rate = 0.01\n",
        "train_batch_size = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrH-tgLqfmSL"
      },
      "source": [
        "# prepare batch of training embedding\n",
        "train_data = data.TensorDataset(train_embeddings_torch,train_target)\n",
        "train_loader = data.DataLoader(dataset=train_data,batch_size=train_batch_size,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42RS78Y51qmS"
      },
      "source": [
        "class SeqNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SeqNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, num_layers=2, batch_first =True, dropout=0.2)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R"
      },
      "source": [
        "### 2.3.3. Train Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "outputId": "7e0de1b9-817e-4933-da18-4d8fed503f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "seq = SeqNet().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(seq.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):  \n",
        "    for batch,(train_embeddings_from_loader,train_target_from_loader) in enumerate(train_loader):\n",
        "      seq.train()\n",
        "      outputs = seq(train_embeddings_from_loader) \n",
        "      loss = criterion(outputs, train_target_from_loader)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      seq.eval()\n",
        "      outputs = seq(train_embeddings_from_loader) \n",
        "      if epoch%100 == 99 and batch == int(train_embeddings.shape[0]/train_batch_size)-1:\n",
        "          loss = criterion(outputs, train_target_from_loader)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          acc= accuracy_score(train_target_from_loader.cpu().numpy(),predicted.cpu().numpy())\n",
        "          f1= f1_score(train_target_from_loader.cpu().numpy(),predicted.cpu().numpy())\n",
        "          print('Epoch: %d, loss: %.5f, train_acc: %.4f, train_f1_score: %.4f' %(epoch + 1, loss.item(), acc, f1))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, loss: 0.69402, train_acc: 0.4960, train_f1_score: 0.0000\n",
            "Epoch: 200, loss: 0.69181, train_acc: 0.5004, train_f1_score: 0.6670\n",
            "Epoch: 300, loss: 0.40075, train_acc: 0.8264, train_f1_score: 0.8148\n",
            "Epoch: 400, loss: 0.19478, train_acc: 0.9420, train_f1_score: 0.9417\n",
            "Epoch: 500, loss: 0.13474, train_acc: 0.9660, train_f1_score: 0.9662\n",
            "Epoch: 600, loss: 0.10610, train_acc: 0.9740, train_f1_score: 0.9742\n",
            "Epoch: 700, loss: 0.09108, train_acc: 0.9774, train_f1_score: 0.9773\n",
            "Epoch: 800, loss: 0.38112, train_acc: 0.8268, train_f1_score: 0.7933\n",
            "Epoch: 900, loss: 0.04826, train_acc: 0.9888, train_f1_score: 0.9888\n",
            "Epoch: 1000, loss: 0.03697, train_acc: 0.9902, train_f1_score: 0.9903\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2"
      },
      "source": [
        "### 2.3.4. Save Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "outputId": "63c9a313-8f46-4c56-a848-7377b45b4a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# save model and upload it to Google Drive\n",
        "torch.save(seq, 'lewu9692_sequence.pt')\n",
        "\n",
        "uploaded = drive.CreateFile({'title': 'lewu9692_sequence.pt'})\n",
        "uploaded.SetContentFile('lewu9692_sequence.pt')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning:\n",
            "\n",
            "Couldn't retrieve source code for container of type SeqNet. It won't be checked for correctness upon loading.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1NDzReLC6Ozn8cUK8LqVUv4Ipgw5zn_gN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3"
      },
      "source": [
        "### 2.3.5. Load Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan"
      },
      "source": [
        "# download model from Google Drive and load it\n",
        "id = '1NDzReLC6Ozn8cUK8LqVUv4Ipgw5zn_gN'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('lewu9692_sequence.pt')\n",
        "sequence_model = torch.load('lewu9692_sequence.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 3 - Evaluation\n",
        "\n",
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 3.1. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJbb5dayL_07"
      },
      "source": [
        "# prepare test embeddings. Please note that this step will cost one hour\n",
        "test_embeddings = generate_embeddings(reviews_test,embeddings)\n",
        "test_target = torch.from_numpy(np.array(label_test_n)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "outputId": "fa0d3381-dd11-4570-b09e-e27897340d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# predict test embeddings in batch to prevent CUDA out of memory\n",
        "test_batch_size = 5000\n",
        "predictions = []\n",
        "for i in np.arange(0,test_embeddings.shape[0],test_batch_size):\n",
        "  test_embeddings_torch = torch.from_numpy(test_embeddings[i:i+test_batch_size]).float().to(device)\n",
        "  seq.eval()\n",
        "  outputs = seq(test_embeddings_torch) \n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  predictions += list(predicted.cpu().numpy())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(label_test_n,predictions))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.77      0.76     12500\n",
            "           1       0.76      0.73      0.75     12500\n",
            "\n",
            "    accuracy                           0.75     25000\n",
            "   macro avg       0.75      0.75      0.75     25000\n",
            "weighted avg       0.75      0.75      0.75     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 3.2. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "outputId": "609ad9c5-2bff-49ec-dfc1-0dc283f3bf4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# as the following displays, the model with learning rate=0.01 is more stable than other two \n",
        "# and its f1 score is relatively higher at epoch 1000.\n",
        "total_epoch_test=2000\n",
        "test_embeddings_parts = torch.from_numpy(test_embeddings[:1000]).float().to(device) #split 1000 batches for hyperparameter testing\n",
        "test_target_parts = test_target[:1000]\n",
        "hypers = {'lr':[],'epoch':[],'f1':[]}\n",
        "for learning_rate in [0.1,0.01,0.001]:\n",
        "  seq = SeqNet().to(device) #initialize model\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(seq.parameters(), lr=learning_rate)\n",
        "  for epoch in range(total_epoch_test):  \n",
        "      for batch,(train_embeddings_from_loader,train_target_from_loader) in enumerate(train_loader):\n",
        "        seq.train()\n",
        "        outputs = seq(train_embeddings_from_loader) \n",
        "        loss = criterion(outputs, train_target_from_loader)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        seq.eval()\n",
        "        outputs = seq(test_embeddings_parts) \n",
        "        if epoch%50 == 49 and batch == int(train_embeddings.shape[0]/train_batch_size)-1:\n",
        "            loss = criterion(outputs, test_target_parts)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            acc= accuracy_score(test_target_parts.cpu().numpy(),predicted.cpu().numpy())\n",
        "            f1= f1_score(test_target_parts.cpu().numpy(),predicted.cpu().numpy())\n",
        "            hypers['lr'].append(learning_rate)\n",
        "            hypers['epoch'].append(epoch)\n",
        "            hypers['f1'].append(f1)\n",
        "            print('lr: %.3f, Epoch: %d, loss: %.5f, train_acc: %.4f, train_f1_score: %.4f' %(learning_rate,epoch + 1, loss.item(), acc, f1))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr: 0.100, Epoch: 50, loss: 0.75798, train_acc: 0.7280, train_f1_score: 0.7349\n",
            "lr: 0.100, Epoch: 100, loss: 0.82571, train_acc: 0.7290, train_f1_score: 0.7451\n",
            "lr: 0.100, Epoch: 150, loss: 0.78348, train_acc: 0.7230, train_f1_score: 0.7287\n",
            "lr: 0.100, Epoch: 200, loss: 0.78476, train_acc: 0.7350, train_f1_score: 0.7347\n",
            "lr: 0.100, Epoch: 250, loss: 0.70280, train_acc: 0.7400, train_f1_score: 0.7426\n",
            "lr: 0.100, Epoch: 300, loss: 0.63881, train_acc: 0.6400, train_f1_score: 0.6797\n",
            "lr: 0.100, Epoch: 350, loss: 0.61520, train_acc: 0.6510, train_f1_score: 0.7197\n",
            "lr: 0.100, Epoch: 400, loss: 0.58936, train_acc: 0.6910, train_f1_score: 0.7147\n",
            "lr: 0.100, Epoch: 450, loss: 0.59130, train_acc: 0.6760, train_f1_score: 0.7282\n",
            "lr: 0.100, Epoch: 500, loss: 0.56660, train_acc: 0.7070, train_f1_score: 0.7196\n",
            "lr: 0.100, Epoch: 550, loss: 0.55281, train_acc: 0.7090, train_f1_score: 0.7215\n",
            "lr: 0.100, Epoch: 600, loss: 0.56558, train_acc: 0.7130, train_f1_score: 0.7139\n",
            "lr: 0.100, Epoch: 650, loss: 0.55144, train_acc: 0.7140, train_f1_score: 0.7317\n",
            "lr: 0.100, Epoch: 700, loss: 0.54317, train_acc: 0.7250, train_f1_score: 0.7358\n",
            "lr: 0.100, Epoch: 750, loss: 0.54203, train_acc: 0.7240, train_f1_score: 0.7449\n",
            "lr: 0.100, Epoch: 800, loss: 0.54502, train_acc: 0.7190, train_f1_score: 0.7391\n",
            "lr: 0.100, Epoch: 850, loss: 0.54129, train_acc: 0.7130, train_f1_score: 0.7156\n",
            "lr: 0.100, Epoch: 900, loss: 0.55697, train_acc: 0.7060, train_f1_score: 0.7278\n",
            "lr: 0.100, Epoch: 950, loss: 0.55948, train_acc: 0.6930, train_f1_score: 0.7171\n",
            "lr: 0.100, Epoch: 1000, loss: 0.68463, train_acc: 0.5390, train_f1_score: 0.6728\n",
            "lr: 0.100, Epoch: 1050, loss: 0.68152, train_acc: 0.5550, train_f1_score: 0.6647\n",
            "lr: 0.100, Epoch: 1100, loss: 0.67909, train_acc: 0.5530, train_f1_score: 0.6631\n",
            "lr: 0.100, Epoch: 1150, loss: 0.66892, train_acc: 0.5790, train_f1_score: 0.6798\n",
            "lr: 0.100, Epoch: 1200, loss: 0.67050, train_acc: 0.5940, train_f1_score: 0.6565\n",
            "lr: 0.100, Epoch: 1250, loss: 0.66293, train_acc: 0.6100, train_f1_score: 0.6890\n",
            "lr: 0.100, Epoch: 1300, loss: 0.64354, train_acc: 0.6310, train_f1_score: 0.6760\n",
            "lr: 0.100, Epoch: 1350, loss: 0.64203, train_acc: 0.6330, train_f1_score: 0.7024\n",
            "lr: 0.100, Epoch: 1400, loss: 0.63657, train_acc: 0.6450, train_f1_score: 0.7024\n",
            "lr: 0.100, Epoch: 1450, loss: 0.62653, train_acc: 0.6560, train_f1_score: 0.7090\n",
            "lr: 0.100, Epoch: 1500, loss: 0.61592, train_acc: 0.6560, train_f1_score: 0.6901\n",
            "lr: 0.100, Epoch: 1550, loss: 0.60367, train_acc: 0.6620, train_f1_score: 0.6998\n",
            "lr: 0.100, Epoch: 1600, loss: 0.59938, train_acc: 0.6830, train_f1_score: 0.7382\n",
            "lr: 0.100, Epoch: 1650, loss: 0.65991, train_acc: 0.5730, train_f1_score: 0.6777\n",
            "lr: 0.100, Epoch: 1700, loss: 0.66481, train_acc: 0.5450, train_f1_score: 0.5390\n",
            "lr: 0.100, Epoch: 1750, loss: 0.65846, train_acc: 0.5830, train_f1_score: 0.7045\n",
            "lr: 0.100, Epoch: 1800, loss: 0.65631, train_acc: 0.5900, train_f1_score: 0.7076\n",
            "lr: 0.100, Epoch: 1850, loss: 0.67274, train_acc: 0.5510, train_f1_score: 0.6927\n",
            "lr: 0.100, Epoch: 1900, loss: 0.69180, train_acc: 0.5300, train_f1_score: 0.6732\n",
            "lr: 0.100, Epoch: 1950, loss: 0.68746, train_acc: 0.5330, train_f1_score: 0.6741\n",
            "lr: 0.100, Epoch: 2000, loss: 0.68705, train_acc: 0.5330, train_f1_score: 0.6718\n",
            "lr: 0.010, Epoch: 50, loss: 0.70391, train_acc: 0.5810, train_f1_score: 0.6871\n",
            "lr: 0.010, Epoch: 100, loss: 0.91449, train_acc: 0.7680, train_f1_score: 0.7642\n",
            "lr: 0.010, Epoch: 150, loss: 1.16336, train_acc: 0.7710, train_f1_score: 0.7783\n",
            "lr: 0.010, Epoch: 200, loss: 1.32114, train_acc: 0.7730, train_f1_score: 0.7681\n",
            "lr: 0.010, Epoch: 250, loss: 1.36099, train_acc: 0.7710, train_f1_score: 0.7770\n",
            "lr: 0.010, Epoch: 300, loss: 1.42080, train_acc: 0.7670, train_f1_score: 0.7704\n",
            "lr: 0.010, Epoch: 350, loss: 1.65038, train_acc: 0.7580, train_f1_score: 0.7531\n",
            "lr: 0.010, Epoch: 400, loss: 1.58616, train_acc: 0.7720, train_f1_score: 0.7725\n",
            "lr: 0.010, Epoch: 450, loss: 1.53068, train_acc: 0.7680, train_f1_score: 0.7730\n",
            "lr: 0.010, Epoch: 500, loss: 1.49635, train_acc: 0.7750, train_f1_score: 0.7702\n",
            "lr: 0.010, Epoch: 550, loss: 1.58152, train_acc: 0.7690, train_f1_score: 0.7645\n",
            "lr: 0.010, Epoch: 600, loss: 1.43703, train_acc: 0.7780, train_f1_score: 0.7798\n",
            "lr: 0.010, Epoch: 650, loss: 1.44189, train_acc: 0.7890, train_f1_score: 0.7941\n",
            "lr: 0.010, Epoch: 700, loss: 1.63140, train_acc: 0.7800, train_f1_score: 0.7843\n",
            "lr: 0.010, Epoch: 750, loss: 1.60270, train_acc: 0.7600, train_f1_score: 0.7576\n",
            "lr: 0.010, Epoch: 800, loss: 1.62528, train_acc: 0.7740, train_f1_score: 0.7802\n",
            "lr: 0.010, Epoch: 850, loss: 1.74002, train_acc: 0.7660, train_f1_score: 0.7583\n",
            "lr: 0.010, Epoch: 900, loss: 1.79078, train_acc: 0.7800, train_f1_score: 0.7822\n",
            "lr: 0.010, Epoch: 950, loss: 1.60021, train_acc: 0.7700, train_f1_score: 0.7928\n",
            "lr: 0.010, Epoch: 1000, loss: 2.03119, train_acc: 0.7720, train_f1_score: 0.7738\n",
            "lr: 0.010, Epoch: 1050, loss: 1.83256, train_acc: 0.7760, train_f1_score: 0.7817\n",
            "lr: 0.010, Epoch: 1100, loss: 1.89759, train_acc: 0.7690, train_f1_score: 0.7640\n",
            "lr: 0.010, Epoch: 1150, loss: 1.85992, train_acc: 0.7790, train_f1_score: 0.7840\n",
            "lr: 0.010, Epoch: 1200, loss: 1.59292, train_acc: 0.7790, train_f1_score: 0.7805\n",
            "lr: 0.010, Epoch: 1250, loss: 1.61015, train_acc: 0.7600, train_f1_score: 0.7605\n",
            "lr: 0.010, Epoch: 1300, loss: 1.90617, train_acc: 0.7590, train_f1_score: 0.7558\n",
            "lr: 0.010, Epoch: 1350, loss: 2.00135, train_acc: 0.7710, train_f1_score: 0.7721\n",
            "lr: 0.010, Epoch: 1400, loss: 1.90866, train_acc: 0.7710, train_f1_score: 0.7748\n",
            "lr: 0.010, Epoch: 1450, loss: 2.23882, train_acc: 0.7670, train_f1_score: 0.7713\n",
            "lr: 0.010, Epoch: 1500, loss: 2.22124, train_acc: 0.7630, train_f1_score: 0.7487\n",
            "lr: 0.010, Epoch: 1550, loss: 1.97563, train_acc: 0.7690, train_f1_score: 0.7711\n",
            "lr: 0.010, Epoch: 1600, loss: 1.86583, train_acc: 0.7760, train_f1_score: 0.7751\n",
            "lr: 0.010, Epoch: 1650, loss: 1.91203, train_acc: 0.7810, train_f1_score: 0.7851\n",
            "lr: 0.010, Epoch: 1700, loss: 2.17427, train_acc: 0.7770, train_f1_score: 0.7759\n",
            "lr: 0.010, Epoch: 1750, loss: 1.80637, train_acc: 0.7920, train_f1_score: 0.7945\n",
            "lr: 0.010, Epoch: 1800, loss: 2.01449, train_acc: 0.7880, train_f1_score: 0.7926\n",
            "lr: 0.010, Epoch: 1850, loss: 1.50821, train_acc: 0.7790, train_f1_score: 0.7844\n",
            "lr: 0.010, Epoch: 1900, loss: 1.84132, train_acc: 0.7850, train_f1_score: 0.7985\n",
            "lr: 0.010, Epoch: 1950, loss: 1.94397, train_acc: 0.7900, train_f1_score: 0.7883\n",
            "lr: 0.010, Epoch: 2000, loss: 2.05177, train_acc: 0.7870, train_f1_score: 0.7846\n",
            "lr: 0.001, Epoch: 50, loss: 0.42880, train_acc: 0.8200, train_f1_score: 0.8211\n",
            "lr: 0.001, Epoch: 100, loss: 0.41304, train_acc: 0.8310, train_f1_score: 0.8338\n",
            "lr: 0.001, Epoch: 150, loss: 0.43468, train_acc: 0.8180, train_f1_score: 0.8312\n",
            "lr: 0.001, Epoch: 200, loss: 0.56827, train_acc: 0.8070, train_f1_score: 0.8056\n",
            "lr: 0.001, Epoch: 250, loss: 0.61335, train_acc: 0.7880, train_f1_score: 0.7819\n",
            "lr: 0.001, Epoch: 300, loss: 0.74066, train_acc: 0.7980, train_f1_score: 0.7960\n",
            "lr: 0.001, Epoch: 350, loss: 0.69303, train_acc: 0.7950, train_f1_score: 0.7964\n",
            "lr: 0.001, Epoch: 400, loss: 0.90969, train_acc: 0.7690, train_f1_score: 0.8047\n",
            "lr: 0.001, Epoch: 450, loss: 0.53456, train_acc: 0.8070, train_f1_score: 0.8150\n",
            "lr: 0.001, Epoch: 500, loss: 0.91303, train_acc: 0.6800, train_f1_score: 0.5897\n",
            "lr: 0.001, Epoch: 550, loss: 0.84653, train_acc: 0.7930, train_f1_score: 0.7915\n",
            "lr: 0.001, Epoch: 600, loss: 0.59812, train_acc: 0.7980, train_f1_score: 0.8031\n",
            "lr: 0.001, Epoch: 650, loss: 0.58430, train_acc: 0.7950, train_f1_score: 0.7915\n",
            "lr: 0.001, Epoch: 700, loss: 0.90245, train_acc: 0.7830, train_f1_score: 0.7806\n",
            "lr: 0.001, Epoch: 750, loss: 0.89308, train_acc: 0.7870, train_f1_score: 0.7876\n",
            "lr: 0.001, Epoch: 800, loss: 0.86408, train_acc: 0.7870, train_f1_score: 0.7851\n",
            "lr: 0.001, Epoch: 850, loss: 0.94973, train_acc: 0.7860, train_f1_score: 0.7821\n",
            "lr: 0.001, Epoch: 900, loss: 1.03061, train_acc: 0.7980, train_f1_score: 0.7968\n",
            "lr: 0.001, Epoch: 950, loss: 0.67551, train_acc: 0.6000, train_f1_score: 0.6276\n",
            "lr: 0.001, Epoch: 1000, loss: 0.67572, train_acc: 0.6000, train_f1_score: 0.6276\n",
            "lr: 0.001, Epoch: 1050, loss: 0.67547, train_acc: 0.6000, train_f1_score: 0.6276\n",
            "lr: 0.001, Epoch: 1100, loss: 0.67573, train_acc: 0.6000, train_f1_score: 0.6276\n",
            "lr: 0.001, Epoch: 1150, loss: 0.67557, train_acc: 0.6000, train_f1_score: 0.6276\n",
            "lr: 0.001, Epoch: 1200, loss: 0.67584, train_acc: 0.6000, train_f1_score: 0.6276\n",
            "lr: 0.001, Epoch: 1250, loss: 0.67325, train_acc: 0.6020, train_f1_score: 0.6287\n",
            "lr: 0.001, Epoch: 1300, loss: 0.68024, train_acc: 0.5910, train_f1_score: 0.6181\n",
            "lr: 0.001, Epoch: 1350, loss: 0.68016, train_acc: 0.5900, train_f1_score: 0.6175\n",
            "lr: 0.001, Epoch: 1400, loss: 0.67992, train_acc: 0.5910, train_f1_score: 0.6188\n",
            "lr: 0.001, Epoch: 1450, loss: 0.68944, train_acc: 0.5070, train_f1_score: 0.3830\n",
            "lr: 0.001, Epoch: 1500, loss: 0.68617, train_acc: 0.5550, train_f1_score: 0.5301\n",
            "lr: 0.001, Epoch: 1550, loss: 0.57616, train_acc: 0.7030, train_f1_score: 0.7411\n",
            "lr: 0.001, Epoch: 1600, loss: 0.47142, train_acc: 0.7760, train_f1_score: 0.7879\n",
            "lr: 0.001, Epoch: 1650, loss: 0.45716, train_acc: 0.7900, train_f1_score: 0.7857\n",
            "lr: 0.001, Epoch: 1700, loss: 0.43386, train_acc: 0.7990, train_f1_score: 0.8065\n",
            "lr: 0.001, Epoch: 1750, loss: 0.44199, train_acc: 0.7970, train_f1_score: 0.8000\n",
            "lr: 0.001, Epoch: 1800, loss: 0.46435, train_acc: 0.7950, train_f1_score: 0.8075\n",
            "lr: 0.001, Epoch: 1850, loss: 0.51425, train_acc: 0.7930, train_f1_score: 0.8008\n",
            "lr: 0.001, Epoch: 1900, loss: 0.59813, train_acc: 0.7840, train_f1_score: 0.8099\n",
            "lr: 0.001, Epoch: 1950, loss: 0.59770, train_acc: 0.7900, train_f1_score: 0.8019\n",
            "lr: 0.001, Epoch: 2000, loss: 0.68749, train_acc: 0.7850, train_f1_score: 0.7907\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlA4BnGSQ5ay",
        "outputId": "0bc46f52-f011-4e86-8422-125829af0e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "import plotly.express as px\n",
        "df = pd.DataFrame(hypers)\n",
        "fig = px.line(df, x=\"epoch\", y=\"f1\", color='lr')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"fefb6f43-f8ae-4d99-b87d-880d66479c9f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"fefb6f43-f8ae-4d99-b87d-880d66479c9f\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'fefb6f43-f8ae-4d99-b87d-880d66479c9f',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"lr=0.1<br>epoch=%{x}<br>f1=%{y}\", \"legendgroup\": \"lr=0.1\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"lr=0.1\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [49, 99, 149, 199, 249, 299, 349, 399, 449, 499, 549, 599, 649, 699, 749, 799, 849, 899, 949, 999, 1049, 1099, 1149, 1199, 1249, 1299, 1349, 1399, 1449, 1499, 1549, 1599, 1649, 1699, 1749, 1799, 1849, 1899, 1949, 1999], \"xaxis\": \"x\", \"y\": [0.7348927875243665, 0.7450611476952023, 0.7286973555337904, 0.7347347347347347, 0.7425742574257426, 0.6797153024911031, 0.7196787148594377, 0.7146814404432134, 0.7281879194630871, 0.7196172248803828, 0.7215311004784689, 0.7138584247258224, 0.7317073170731707, 0.7358309317963496, 0.744916820702403, 0.7390900649953573, 0.7155599603567889, 0.7277777777777779, 0.7170506912442396, 0.6728176011355571, 0.6646571213262998, 0.6631499623210249, 0.6798479087452471, 0.6565143824027072, 0.6889952153110048, 0.6760316066725197, 0.70235198702352, 0.7024308466051971, 0.7089678510998307, 0.69009009009009, 0.6998223801065719, 0.7382328654004955, 0.6777358490566038, 0.5390070921985815, 0.7044649184975196, 0.7075606276747504, 0.6926762491444216, 0.6731571627260083, 0.6741102581995814, 0.6718200983836964], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"lr=0.01<br>epoch=%{x}<br>f1=%{y}\", \"legendgroup\": \"lr=0.01\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"lr=0.01\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [49, 99, 149, 199, 249, 299, 349, 399, 449, 499, 549, 599, 649, 699, 749, 799, 849, 899, 949, 999, 1049, 1099, 1149, 1199, 1249, 1299, 1349, 1399, 1449, 1499, 1549, 1599, 1649, 1699, 1749, 1799, 1849, 1899, 1949, 1999], \"xaxis\": \"x\", \"y\": [0.6870799103808812, 0.7642276422764227, 0.7783155856727977, 0.7681307456588355, 0.7770204479065239, 0.7704433497536946, 0.7530612244897958, 0.7724550898203593, 0.7729941291585127, 0.7701736465781409, 0.7645259938837919, 0.7797619047619049, 0.7941463414634147, 0.7843137254901961, 0.7575757575757576, 0.7801556420233463, 0.7582644628099173, 0.7821782178217822, 0.7927927927927928, 0.7738095238095237, 0.7816764132553606, 0.7640449438202247, 0.7839687194525904, 0.7805362462760675, 0.7604790419161676, 0.7558257345491388, 0.772139303482587, 0.7748279252704032, 0.7713444553483808, 0.7486744432661717, 0.7710604558969276, 0.7751004016064258, 0.7850834151128558, 0.7758793969849247, 0.7944664031620553, 0.7925636007827789, 0.7843902439024389, 0.7985004686035614, 0.7883064516129032, 0.7846309403437816], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"lr=0.001<br>epoch=%{x}<br>f1=%{y}\", \"legendgroup\": \"lr=0.001\", \"line\": {\"color\": \"#00cc96\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"lr=0.001\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [49, 99, 149, 199, 249, 299, 349, 399, 449, 499, 549, 599, 649, 699, 749, 799, 849, 899, 949, 999, 1049, 1099, 1149, 1199, 1249, 1299, 1349, 1399, 1449, 1499, 1549, 1599, 1649, 1699, 1749, 1799, 1849, 1899, 1949, 1999], \"xaxis\": \"x\", \"y\": [0.8210735586481114, 0.8338249754178957, 0.8311688311688311, 0.8056394763343403, 0.7818930041152262, 0.7959595959595959, 0.7964250248262165, 0.8047337278106509, 0.8149568552253117, 0.5897435897435896, 0.7915407854984894, 0.8031189083820662, 0.7914547304170906, 0.7805864509605663, 0.7876370887337987, 0.7850655903128154, 0.7820773930753564, 0.7967806841046278, 0.6275605214152701, 0.6275605214152701, 0.6275605214152701, 0.6275605214152701, 0.6275605214152701, 0.6275605214152701, 0.628731343283582, 0.6181139122315594, 0.6175373134328358, 0.6188257222739981, 0.38297872340425526, 0.5300950369588173, 0.7410636442894508, 0.7878787878787878, 0.7857142857142858, 0.8065447545717035, 0.7999999999999999, 0.8075117370892019, 0.8007699711260826, 0.8098591549295774, 0.8018867924528302, 0.7906523855890945], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"f1\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fefb6f43-f8ae-4d99-b87d-880d66479c9f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVmx4E52dXS"
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}